{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f1829f-fb33-4f10-9fe0-99bfb07402fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For data visualization using plots\n",
    "import seaborn as sns  # For advanced data visualization\n",
    "\n",
    "# Import preprocessing and machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler  # Standardizes features by scaling\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree algorithm for classification\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier  # Random Forest and AdaBoost ensemble classifiers\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
    "from sklearn.model_selection import KFold, LeaveOneGroupOut  # For cross-validation strategies\n",
    "from sklearn.metrics import (  # For model performance evaluation\n",
    "    accuracy_score,  # To calculate model accuracy\n",
    "    precision_recall_fscore_support,  # For precision, recall, and F1-score metrics\n",
    "    classification_report  # To generate a detailed performance report\n",
    ")\n",
    "\n",
    "# Code functionality:\n",
    "# 1. Importing necessary libraries for data manipulation, visualization, preprocessing, and machine learning.\n",
    "# 2. Provides tools for building classifiers, evaluating model performance, and performing cross-validation.\n",
    "\n",
    "# Next steps would involve loading the dataset, preprocessing the data, training models,\n",
    "# and evaluating their performance using metrics and visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205599d0-f17b-4e91-b4f9-5c8e4a1af2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (features) from a text file\n",
    "X_train = np.loadtxt(\"UCI HAR Dataset/train/X_train.txt\")  \n",
    "# X_train contains the feature values for the training set.\n",
    "\n",
    "# Load the training labels (target/output) from a text file and convert them to integers\n",
    "y_train = np.loadtxt(\"UCI HAR Dataset/train/y_train.txt\").astype(int)  \n",
    "# y_train contains the activity labels corresponding to X_train.\n",
    "\n",
    "# Load the test data (features) from a text file\n",
    "X_test = np.loadtxt(\"UCI HAR Dataset/test/X_test.txt\")  \n",
    "# X_test contains the feature values for the test set.\n",
    "\n",
    "# Load the test labels (target/output) from a text file and convert them to integers\n",
    "y_test = np.loadtxt(\"UCI HAR Dataset/test/y_test.txt\").astype(int)  \n",
    "# y_test contains the activity labels corresponding to X_test.\n",
    "\n",
    "# Load the subject identifiers (who performed the activities) for the training data\n",
    "subjects = np.loadtxt(\"UCI HAR Dataset/train/subject_train.txt\").astype(int)  \n",
    "# subjects contains the IDs of individuals performing the activities in the training set.\n",
    "\n",
    "# Overall:\n",
    "# - The data comes from the UCI Human Activity Recognition (HAR) dataset.\n",
    "# - `X_train` and `X_test` contain the features for training and testing, respectively.\n",
    "# - `y_train` and `y_test` contain the corresponding activity labels (target classes).\n",
    "# - `subjects` gives information about the subjects involved in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3326b15c-b7b6-4ae7-9a72-26057f81e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7352, 561), y_train shape: (7352,)\n",
      "X_test shape: (2947, 561), y_test shape: (2947,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape (dimensions) of the training features and labels\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")  \n",
    "# X_train.shape gives the number of samples (rows) and features (columns) in the training dataset.\n",
    "# y_train.shape gives the number of labels (rows) in the training dataset.\n",
    "\n",
    "# Print the shape (dimensions) of the test features and labels\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")  \n",
    "# X_test.shape gives the number of samples (rows) and features (columns) in the test dataset.\n",
    "# y_test.shape gives the number of labels (rows) in the test dataset.\n",
    "\n",
    "# Verification step:\n",
    "# - Check if the data has been loaded correctly and examine the dimensions of the datasets.\n",
    "# - Ensure that the number of labels matches the number of samples in both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da097fa-aa76-4230-9a3e-3170d87e3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the training data using fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalize the test data using transform (without fitting again)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e6f5b2-4fed-4371-aa0a-6674d986f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store training and testing accuracy\n",
    "training_acc = []\n",
    "testing_acc = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1991a8e0-8e5e-4013-b3c5-65a1c1480556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 561, Reduced features: 202\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train)\n",
    "correlation_matrix = X_train_df.corr().abs()\n",
    "\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "X_train_reduced = X_train_df.drop(columns=to_drop)\n",
    "X_test_reduced = pd.DataFrame(X_test).drop(columns=to_drop)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}, Reduced features: {X_train_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f09a6c2-f389-47e6-90c7-d14bd18ddf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148061</td>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.006290</td>\n",
       "      <td>0.022754</td>\n",
       "      <td>0.047558</td>\n",
       "      <td>0.044062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030681</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>0.015613</td>\n",
       "      <td>0.544320</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>0.037444</td>\n",
       "      <td>0.028844</td>\n",
       "      <td>0.035257</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.028242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078769</td>\n",
       "      <td>0.045160</td>\n",
       "      <td>0.044920</td>\n",
       "      <td>0.049746</td>\n",
       "      <td>0.044180</td>\n",
       "      <td>0.045049</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.038108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022395</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.070559</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.013903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.078769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.018747</td>\n",
       "      <td>0.015203</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.037197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.019127</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.022643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.045160</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927461</td>\n",
       "      <td>0.851668</td>\n",
       "      <td>0.998632</td>\n",
       "      <td>0.920888</td>\n",
       "      <td>0.846392</td>\n",
       "      <td>0.980844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065987</td>\n",
       "      <td>0.148034</td>\n",
       "      <td>0.115565</td>\n",
       "      <td>0.035011</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.024810</td>\n",
       "      <td>0.371653</td>\n",
       "      <td>0.471065</td>\n",
       "      <td>0.394825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021903</td>\n",
       "      <td>0.044920</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>0.927461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895510</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.894509</td>\n",
       "      <td>0.917366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105621</td>\n",
       "      <td>0.206227</td>\n",
       "      <td>0.176946</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.433169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.037444</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>0.023525</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>0.034514</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116001</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.012313</td>\n",
       "      <td>0.019903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0.028844</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>0.024810</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>0.116001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023995</td>\n",
       "      <td>0.005869</td>\n",
       "      <td>0.005656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0.035257</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.371653</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>0.345011</td>\n",
       "      <td>0.368191</td>\n",
       "      <td>0.377025</td>\n",
       "      <td>0.347389</td>\n",
       "      <td>0.384192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087332</td>\n",
       "      <td>0.086006</td>\n",
       "      <td>0.079751</td>\n",
       "      <td>0.011880</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.023995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783848</td>\n",
       "      <td>0.643655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.471065</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.476006</td>\n",
       "      <td>0.466424</td>\n",
       "      <td>0.525081</td>\n",
       "      <td>0.477607</td>\n",
       "      <td>0.480229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100125</td>\n",
       "      <td>0.086993</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>0.012313</td>\n",
       "      <td>0.005869</td>\n",
       "      <td>0.783848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.594885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.028242</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.394825</td>\n",
       "      <td>0.433169</td>\n",
       "      <td>0.482828</td>\n",
       "      <td>0.390922</td>\n",
       "      <td>0.431459</td>\n",
       "      <td>0.479751</td>\n",
       "      <td>0.405023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057468</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>0.052548</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.019903</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>0.643655</td>\n",
       "      <td>0.594885</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    1.000000  0.148061  0.256952  0.000619  0.021903  0.044617  0.006290   \n",
       "1    0.148061  1.000000  0.078769  0.045160  0.044920  0.049746  0.044180   \n",
       "2    0.256952  0.078769  1.000000  0.020217  0.016641  0.008410  0.018747   \n",
       "3    0.000619  0.045160  0.020217  1.000000  0.927461  0.851668  0.998632   \n",
       "4    0.021903  0.044920  0.016641  0.927461  1.000000  0.895510  0.922803   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "556  0.037444  0.017967  0.063609  0.018985  0.008507  0.018429  0.019389   \n",
       "557  0.028844  0.075679  0.034037  0.024810  0.014592  0.006471  0.024951   \n",
       "558  0.035257  0.005309  0.008587  0.371653  0.380531  0.345011  0.368191   \n",
       "559  0.034371  0.001053  0.015288  0.471065  0.523600  0.476006  0.466424   \n",
       "560  0.028242  0.013903  0.022643  0.394825  0.433169  0.482828  0.390922   \n",
       "\n",
       "          7         8         9    ...       551       552       553  \\\n",
       "0    0.022754  0.047558  0.044062  ...  0.030681  0.017557  0.015613   \n",
       "1    0.045049  0.050402  0.038108  ...  0.022395  0.001587  0.004459   \n",
       "2    0.015203  0.001988  0.037197  ...  0.020481  0.020091  0.019127   \n",
       "3    0.920888  0.846392  0.980844  ...  0.065987  0.148034  0.115565   \n",
       "4    0.997347  0.894509  0.917366  ...  0.105621  0.206227  0.176946   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "556  0.012546  0.023525  0.025066  ...  0.026615  0.034514  0.024553   \n",
       "557  0.012341  0.007231  0.028871  ...  0.000102  0.017937  0.014865   \n",
       "558  0.377025  0.347389  0.384192  ...  0.087332  0.086006  0.079751   \n",
       "559  0.525081  0.477607  0.480229  ...  0.100125  0.086993  0.078079   \n",
       "560  0.431459  0.479751  0.405023  ...  0.057468  0.057831  0.052548   \n",
       "\n",
       "          554       555       556       557       558       559       560  \n",
       "0    0.544320  0.012173  0.037444  0.028844  0.035257  0.034371  0.028242  \n",
       "1    0.070559  0.013541  0.017967  0.075679  0.005309  0.001053  0.013903  \n",
       "2    0.052841  0.039836  0.063609  0.034037  0.008587  0.015288  0.022643  \n",
       "3    0.035011  0.021633  0.018985  0.024810  0.371653  0.471065  0.394825  \n",
       "4    0.020379  0.012505  0.008507  0.014592  0.380531  0.523600  0.433169  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "556  0.006269  0.009141  1.000000  0.116001  0.005853  0.012313  0.019903  \n",
       "557  0.020823  0.035263  0.116001  1.000000  0.023995  0.005869  0.005656  \n",
       "558  0.011880  0.023246  0.005853  0.023995  1.000000  0.783848  0.643655  \n",
       "559  0.001540  0.012990  0.012313  0.005869  0.783848  1.000000  0.594885  \n",
       "560  0.003069  0.017520  0.019903  0.005656  0.643655  0.594885  1.000000  \n",
       "\n",
       "[561 rows x 561 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73be7b0-7b9d-44c8-b9a8-fd5c2b0c38c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 561, Reduced features: 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teekam Singh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Teekam Singh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Teekam Singh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Teekam Singh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  Accuracy: 0.9780\n",
      "  Precision: 0.9780\n",
      "  Recall: 0.9780\n",
      "  F1: 0.9780\n",
      "Decision Tree:\n",
      "  Accuracy: 0.9358\n",
      "  Precision: 0.9361\n",
      "  Recall: 0.9358\n",
      "  F1: 0.9358\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9740\n",
      "  Precision: 0.9741\n",
      "  Recall: 0.9740\n",
      "  F1: 0.9740\n",
      "AdaBoost:\n",
      "  Accuracy: 0.3890\n",
      "  Precision: 0.5334\n",
      "  Recall: 0.3890\n",
      "  F1: 0.2430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teekam Singh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Normalize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assuming X_train and X_test are NumPy arrays; Normalize them\n",
    "X_train_normalized = scaler.fit_transform(X_train)  # Fit and transform training data\n",
    "X_test_normalized = scaler.transform(X_test)  # Transform test data\n",
    "\n",
    "# Step 2: Convert to pandas DataFrame to use iloc and calculate correlation\n",
    "X_train_df = pd.DataFrame(X_train_normalized)\n",
    "X_test_df = pd.DataFrame(X_test_normalized)\n",
    "\n",
    "# Step 3: Calculate the correlation matrix\n",
    "correlation_matrix = X_train_df.corr().abs()\n",
    "\n",
    "# Step 4: Get the upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Step 5: Identify columns with correlation greater than 0.9\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "# Step 6: Create reduced datasets by dropping highly correlated features\n",
    "X_train_reduced = X_train_df.drop(columns=to_drop)\n",
    "X_test_reduced = X_test_df.drop(columns=to_drop)\n",
    "\n",
    "# Print the number of original and reduced features\n",
    "print(f\"Original features: {X_train.shape[1]}, Reduced features: {X_train_reduced.shape[1]}\")\n",
    "\n",
    "# Step 7: List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Step 8: Initialize K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 9: Store results for each model\n",
    "results = {model_name: {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []} for model_name in models.keys()}\n",
    "\n",
    "# Step 10: Evaluate each model using K-Fold Cross-Validation\n",
    "for model_name, model in models.items():\n",
    "    for train_index, test_index in kf.split(X_train_reduced):\n",
    "        # Split data using NumPy indexing\n",
    "        X_train_fold, X_test_fold = X_train_reduced.iloc[train_index], X_train_reduced.iloc[test_index]\n",
    "        \n",
    "        # Use NumPy indexing for y_train (No .iloc method available)\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "        # Compute performance metrics for each fold\n",
    "        results[model_name][\"accuracy\"].append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "        results[model_name][\"precision\"].append(precision_score(y_test_fold, y_pred_fold, average='weighted'))\n",
    "        results[model_name][\"recall\"].append(recall_score(y_test_fold, y_pred_fold, average='weighted'))\n",
    "        results[model_name][\"f1\"].append(f1_score(y_test_fold, y_pred_fold, average='weighted'))\n",
    "\n",
    "# Step 11: Compute average metrics for each model\n",
    "for model_name in results.keys():\n",
    "    for metric in results[model_name].keys():\n",
    "        results[model_name][metric] = np.mean(results[model_name][metric])\n",
    "\n",
    "# Step 12: Print the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14306950-f227-4831-9c3f-babd673815c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
